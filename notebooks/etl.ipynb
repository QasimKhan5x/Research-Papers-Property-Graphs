{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import csv\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import faker\n",
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"en\"\n",
    "max_ngram_size = 2\n",
    "deduplication_threshold = 0.5\n",
    "deduplication_algo = \"seqm\"\n",
    "windowSize = 5\n",
    "numOfKeywords = 3\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor(\n",
    "    lan=language,\n",
    "    n=max_ngram_size,\n",
    "    dedupLim=deduplication_threshold,\n",
    "    dedupFunc=deduplication_algo,\n",
    "    windowsSize=windowSize,\n",
    "    top=numOfKeywords,\n",
    ")\n",
    "\n",
    "keywords = []\n",
    "for abstract in tqdm(df['abstract'].tolist()):\n",
    "    results = kw_extractor.extract_keywords(abstract)\n",
    "    keywords.append([result[0] for result in results])\n",
    "df['keywords'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_data_with_keywords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_data_with_keywords.csv')\n",
    "# Convert string representations to actual data structures\n",
    "df['publicationVenue'] = df['publicationVenue'].apply(ast.literal_eval)\n",
    "df['journal'] = df['journal'].apply(ast.literal_eval)\n",
    "df['authors'] = df['authors'].apply(ast.literal_eval)\n",
    "df['keywords'] = df['keywords'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['paperId'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim to 100 characters\n",
    "df['abstract'] = df['abstract'].apply(lambda x: x[:100])\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h9/fg7r3_v97d5916_gtz3qfnl40000gn/T/ipykernel_41229/1333908408.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  papers.rename(columns={'paperId': 'paperID'}, inplace=True)\n",
      "/var/folders/h9/fg7r3_v97d5916_gtz3qfnl40000gn/T/ipykernel_41229/1333908408.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  papers.dropna(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 120789 entries, 0 to 120813\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   paperID          120789 non-null  object\n",
      " 1   title            120789 non-null  object\n",
      " 2   abstract         120789 non-null  object\n",
      " 3   publicationDate  120789 non-null  object\n",
      " 4   year             120789 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "papers = df[['paperId', 'title', 'abstract', 'publicationDate', 'year']]\n",
    "papers.rename(columns={'paperId': 'paperID'}, inplace=True)\n",
    "papers.loc[:,'abstract'] = papers['abstract'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "papers.loc[:, 'title'] = papers['title'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "papers.dropna(inplace=True)\n",
    "papers.to_csv('papers.csv', index=False)\n",
    "papers = pd.read_csv('papers.csv')\n",
    "papers.dropna(inplace=True)\n",
    "papers.to_csv('papers.csv', index=False)\n",
    "papers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = df['keywords'].explode().dropna().unique()\n",
    "with open(\"keywords.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['keyword'])\n",
    "    for keyword in keywords:\n",
    "        writer.writerow([keyword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 362301 entries, 0 to 276083\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   paperId   362301 non-null  object\n",
      " 1   keywords  362301 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.3+ MB\n"
     ]
    }
   ],
   "source": [
    "keyword_mapping = df[['paperId', 'keywords']].explode('keywords').dropna()\n",
    "keyword_mapping.info()\n",
    "keyword_mapping.to_csv('keyword_mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to get random cited paper IDs\n",
    "# def get_random_cited_papers(paper_id, all_paper_ids, min_citations=1, max_citations=10):\n",
    "#     # Ensure the number of citations does not exceed the number of available papers\n",
    "#     max_citations = min(max_citations, len(all_paper_ids) - 1)\n",
    "#     num_citations = np.random.randint(min_citations, max_citations + 1)\n",
    "#     # Randomly select cited papers, excluding the current paper\n",
    "#     cited_papers = np.random.choice([pid for pid in all_paper_ids if pid != paper_id], num_citations, replace=False)\n",
    "#     return cited_papers\n",
    "\n",
    "# # List to hold tuples of (paperId, citedPaperId)\n",
    "# citations_list = []\n",
    "\n",
    "# # Extract all paper IDs as a list\n",
    "# all_paper_ids = df['paperId'].tolist()\n",
    "\n",
    "# # Generate cited papers for each paperId\n",
    "# for paper_id in tqdm(all_paper_ids):\n",
    "#     cited_paper_ids = get_random_cited_papers(paper_id, all_paper_ids)\n",
    "#     for cited_paper_id in cited_paper_ids:\n",
    "#         citations_list.append((paper_id, cited_paper_id))\n",
    "\n",
    "# # Create a new dataframe for the citations\n",
    "# citations_df = pd.DataFrame(citations_list, columns=['paperId', 'citedPaperId'])\n",
    "\n",
    "# # Write the dataframe to a CSV file\n",
    "# citations_df.to_csv('citations.csv', index=False)\n",
    "# citations_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 404217 entries, 0 to 436911\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   paperID      404217 non-null  object\n",
      " 1   referenceID  404217 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 9.3+ MB\n"
     ]
    }
   ],
   "source": [
    "with open('references.json') as f:\n",
    "    references = json.load(f)\n",
    "\n",
    "citations = []\n",
    "for paper in references:\n",
    "    paper_id = paper['paperId']\n",
    "    for reference in paper['references']:\n",
    "        if reference['paperId'] is not None:\n",
    "            citations.append((paper_id, reference['paperId']))\n",
    "    for citation in paper['citations']:\n",
    "        if citation['paperId'] is not None:\n",
    "            citations.append((citation['paperId'], paper_id))\n",
    "references_df = pd.DataFrame(citations, columns=['paperID', 'referenceID'])\n",
    "references_df.drop_duplicates(inplace=True)\n",
    "references_df.info()\n",
    "references_df.to_csv('citations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 241051 entries, 0 to 241947\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   authorID  241051 non-null  object\n",
      " 1   name      241051 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "from pandas import json_normalize\n",
    "\n",
    "# Extract the authors column, which contains nested JSON-like data\n",
    "# Normalize this data to create a separate DataFrame for authors\n",
    "authors_expanded = json_normalize(df['authors'].explode())  # Normalize and explode the nested data\n",
    "\n",
    "# Create the authors DataFrame with unique authors (drop duplicates)\n",
    "authors_df = authors_expanded[['authorId', 'name']].drop_duplicates().reset_index(drop=True)\n",
    "authors_df.dropna(inplace=True)\n",
    "authors_df.rename(columns={'authorId': 'authorID'}, inplace=True)\n",
    "\n",
    "authors_df.info()\n",
    "authors_df.to_csv('authors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 540478 entries, 0 to 543131\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   paperID      540478 non-null  object\n",
      " 1   authorID     540478 non-null  object\n",
      " 2   corresponds  540478 non-null  bool  \n",
      "dtypes: bool(1), object(2)\n",
      "memory usage: 12.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Expand the DataFrame to include one row per author per paper\n",
    "expanded_df = df.explode('authors')\n",
    "\n",
    "# Extract paperID and authorID\n",
    "expanded_df['paperID'] = expanded_df['paperId']\n",
    "expanded_df['authorID'] = expanded_df['authors'].apply(lambda x: x['authorId'])\n",
    "\n",
    "# Initialize 'corresponds' to False\n",
    "expanded_df['corresponds'] = False\n",
    "\n",
    "expanded_df.loc[expanded_df.groupby('paperId').cumcount() == 0, 'corresponds'] = True\n",
    "\n",
    "# Now, we can create the 'writes' DataFrame with just the necessary columns\n",
    "writes_df = expanded_df[['paperId', 'authorID', 'corresponds']].rename(columns={'paperId': 'paperID'}).reset_index(drop=True)\n",
    "writes_df.drop_duplicates(subset=['paperID', 'authorID'], inplace=True)\n",
    "writes_df.dropna(inplace=True)\n",
    "writes_df.to_csv('writes.csv', index=False)\n",
    "\n",
    "writes_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 'journal' and 'publicationVenue' fields. if 'publicationVenue'.name.lower() == arxiv then ignore\n",
    "\n",
    "1. Journal: use publicationVenue.'name' and journal.'volume'. if either not present then ignore; publicationVenue.type == 'journal'\n",
    "2. Conference: use 'name'. publicationVenue.type == 'conference'\n",
    "3. Same as conference but either journal.name or publicationVenue.name contains 'workshop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Faker generator\n",
    "fake = faker.Faker()\n",
    "\n",
    "# Generate a random list of 50 names\n",
    "random_names = [fake.name() for _ in range(100)]\n",
    "random_cities = [fake.city() for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1479 entries, 0 to 1491\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   journalID  1479 non-null   object\n",
      " 1   name       1479 non-null   object\n",
      " 2   issn       1479 non-null   object\n",
      " 3   editor     1479 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 57.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Journals\n",
    "# name is neither empty nor is equal to arxiv\n",
    "# volume should not be None and it should be a numbeer\n",
    "journals_cond = (df[\"publicationVenue\"].apply(lambda x: x.get(\"type\")) == \"journal\") & (\n",
    "    df[\"journal\"].apply(\n",
    "        lambda x: x.get(\"name\", \"\") != \"\"\n",
    "        and x.get(\"name\").lower() != \"arxiv\"\n",
    "        and x.get(\"volume\", \"\") != \"\"\n",
    "        and x.get(\"volume\").isdigit()\n",
    "    )\n",
    ")\n",
    "journals_data = df[journals_cond]\n",
    "\n",
    "# Extracting journal information\n",
    "journals = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"journalID\": journals_data[\"publicationVenue\"].apply(lambda x: x.get(\"id\")),\n",
    "            \"name\": journals_data[\"journal\"].apply(lambda x: x.get(\"name\")),\n",
    "            \"issn\": journals_data[\"publicationVenue\"].apply(lambda x: x.get(\"issn\")),\n",
    "        }\n",
    "    )\n",
    "    .drop_duplicates(subset=[\"journalID\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "journals.dropna(inplace=True)\n",
    "journals[\"editor\"] = np.random.choice(random_names, size=len(journals))\n",
    "journals.info()\n",
    "journals.to_csv(\"journals.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1242 entries, 0 to 1241\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   conferenceID  1242 non-null   object\n",
      " 1   name          1242 non-null   object\n",
      " 2   chair         1242 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 29.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Conferences\n",
    "# name is neither empty nor contains 'workshop' or 'symposium' or 'arxiv'\n",
    "conferences_cond = (\n",
    "    df[\"publicationVenue\"].apply(lambda x: x.get(\"type\")) == \"conference\"\n",
    ") & df[\"journal\"].apply(\n",
    "    lambda x: x.get(\"name\") is not None\n",
    "    and \"workshop\" not in x.get(\"name\").lower()\n",
    "    and \"symposium\" not in x.get(\"name\").lower()\n",
    "    and \"arxiv\" not in x.get(\"name\").lower()\n",
    ")\n",
    "conferences_data = df[conferences_cond]\n",
    "\n",
    "# Extracting conference information\n",
    "conferences = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"conferenceID\": conferences_data[\"publicationVenue\"].apply(\n",
    "                lambda x: x.get(\"id\")\n",
    "            ),\n",
    "            \"name\": conferences_data[\"publicationVenue\"].apply(lambda x: x.get(\"name\")),\n",
    "        }\n",
    "    )\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "conferences[\"chair\"] = np.random.choice(random_names, size=len(conferences))\n",
    "conferences.info()\n",
    "conferences.to_csv(\"conferences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 326 entries, 0 to 325\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   workshopID  326 non-null    object\n",
      " 1   name        326 non-null    object\n",
      " 2   chair       326 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 7.8+ KB\n"
     ]
    }
   ],
   "source": [
    "workshops_cond = (\n",
    "    df[\"publicationVenue\"].apply(lambda x: x.get(\"type\")) == \"conference\"\n",
    ") & df[\"journal\"].apply(\n",
    "    lambda x: x.get(\"name\") is not None\n",
    "    and (\"workshop\" in x.get(\"name\").lower()\n",
    "    or \"symposium\" in x.get(\"name\").lower())\n",
    ")\n",
    "workshops_data = df[workshops_cond]\n",
    "# select indices not in conferences\n",
    "workshops_data = workshops_data[~workshops_data.index.isin(conferences_data.index)]\n",
    "\n",
    "# Extracting workshop information\n",
    "workshops = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"workshopID\": workshops_data[\"publicationVenue\"].apply(\n",
    "                lambda x: x.get(\"id\")\n",
    "            ),\n",
    "            \"name\": workshops_data[\"publicationVenue\"].apply(lambda x: x.get(\"name\")),\n",
    "        }\n",
    "    )\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "workshops[\"chair\"] = np.random.choice(random_names, size=len(workshops))\n",
    "workshops.to_csv('workshops.csv', index=False)\n",
    "workshops.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate a random publication date in a given year in the format YYYY-MM-DD\n",
    "def random_date(year):\n",
    "    start_date = datetime(year, 1, 1)\n",
    "    end_date = datetime(year, 12, 31)\n",
    "    time_between_dates = end_date - start_date\n",
    "    days_between_dates = time_between_dates.days\n",
    "    random_number_of_days = np.random.randint(0, days_between_dates)\n",
    "    random_date = start_date + pd.Timedelta(days=random_number_of_days)\n",
    "    return random_date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5446 entries, 0 to 25948\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   volID      5446 non-null   object\n",
      " 1   volNumber  5446 non-null   object\n",
      " 2   journalID  5446 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 170.2+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25952 entries, 0 to 25951\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   paperID  25952 non-null  object\n",
      " 1   volID    25952 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 405.6+ KB\n"
     ]
    }
   ],
   "source": [
    "journals_volume_data = journals_data.copy()\n",
    "journals_volume_data[\"volNumber\"] = journals_volume_data[\"journal\"].apply(\n",
    "    lambda x: x.get(\"volume\")\n",
    ")\n",
    "journals_volume_data[\"journalID\"] = journals_volume_data[\"publicationVenue\"].apply(\n",
    "    lambda x: x.get(\"id\")\n",
    ")\n",
    "journals_volume_data[\"volID\"] = (\n",
    "    journals_volume_data[\"volNumber\"].astype(str)\n",
    "    + \"_\"\n",
    "    + journals_volume_data[\"journalID\"]\n",
    ")\n",
    "journals_volume_data[\"publicationDate\"] = journals_volume_data[\"year\"].apply(\n",
    "    lambda x: random_date(int(x))\n",
    ")\n",
    "\n",
    "volume_df = journals_volume_data[[\"volID\", \"volNumber\", \"journalID\"]].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "paper_volume_df = (\n",
    "    journals_volume_data[[\"paperId\", \"volID\"]]\n",
    "    .rename(columns={\"paperId\": \"paperID\"})\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "volume_df.drop_duplicates(subset=[\"volID\"], inplace=True)\n",
    "\n",
    "volume_df.info()\n",
    "paper_volume_df.info()\n",
    "\n",
    "volume_df.to_csv(\"volumes.csv\", index=False)\n",
    "paper_volume_df.to_csv(\"paper_published_in_volume.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conferences_cond = (\n",
    "    df[\"publicationVenue\"].apply(lambda x: x.get(\"type\")) == \"conference\"\n",
    ") & df[\"journal\"].apply(\n",
    "    lambda x: x.get(\"name\") is not None\n",
    "    and \"workshop\" not in x.get(\"name\").lower()\n",
    "    and \"symposium\" not in x.get(\"name\").lower()\n",
    "    and \"arxiv\" not in x.get(\"name\").lower()\n",
    "    and \"corr\" not in x.get(\"name\").lower()\n",
    "    and \"abs/\" not in x.get(\"volume\", \"\").lower()\n",
    ")\n",
    "conferences_data = df[conferences_cond]\n",
    "\n",
    "workshops_cond = (\n",
    "    df[\"publicationVenue\"].apply(lambda x: x.get(\"type\")) == \"conference\"\n",
    ") & df[\"journal\"].apply(\n",
    "    lambda x: x.get(\"name\") is not None\n",
    "    and (\"workshop\" in x.get(\"name\").lower()\n",
    "    or \"symposium\" in x.get(\"name\").lower())\n",
    ")\n",
    "workshops_data = df[workshops_cond]\n",
    "workshops_data = workshops_data[~workshops_data.index.isin(conferences_data.index)]\n",
    "\n",
    "# Proceedings\n",
    "proceedings_data = pd.concat([conferences_data, workshops_data], ignore_index=True)\n",
    "proceedings_data['conferenceID'] = proceedings_data['publicationVenue'].apply(lambda x: x.get('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract edition using regex\n",
    "def extract_edition(name):\n",
    "    match = re.search(r'(\\d+)(st|nd|rd|th)', name)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        match = re.search(r'\\b\\d{4}\\b', name)\n",
    "        if match:\n",
    "            return int(match.group())\n",
    "    return np.nan\n",
    "\n",
    "# Function to generate random start and end dates within the given year\n",
    "def generate_dates(year):\n",
    "    start_date = datetime.strptime(f\"{year}-01-01\", \"%Y-%m-%d\") + pd.to_timedelta(np.random.randint(0, 365), 'D')\n",
    "    end_date = start_date + pd.to_timedelta(np.random.randint(1, 5), 'D')  # End date is 1 to 5 days after start date\n",
    "    return start_date, end_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39026 entries, 0 to 39025\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   paperID        39026 non-null  object\n",
      " 1   proceedingsID  39026 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 609.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5007 entries, 0 to 41374\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   proceedingsID  5007 non-null   object        \n",
      " 1   edition        5007 non-null   int64         \n",
      " 2   conferenceID   5007 non-null   object        \n",
      " 3   type           5007 non-null   object        \n",
      " 4   venue          5007 non-null   object        \n",
      " 5   startDate      5007 non-null   datetime64[ns]\n",
      " 6   endDate        5007 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(1), object(4)\n",
      "memory usage: 312.9+ KB\n"
     ]
    }
   ],
   "source": [
    "proceedings_data['edition'] = proceedings_data['journal'].apply(lambda x: extract_edition(x.get('name')) if x and 'name' in x else np.nan)\n",
    "proceedings_data.loc[proceedings_data['edition'].isna(), 'edition'] = proceedings_data['journal'].apply(lambda x: int(x.get('volume')) if x and 'volume' in x and x.get('volume').isdigit() else np.nan)\n",
    "\n",
    "proceedings_data = proceedings_data.dropna(subset=['edition'])\n",
    "proceedings_data['proceedingsID'] = proceedings_data['edition'].astype(int).astype(str) + \"_\" + proceedings_data['conferenceID']\n",
    "proceedings_data.loc[0:conferences_data.shape[0], 'type'] = 'conference'\n",
    "proceedings_data.loc[conferences_data.shape[0]:, 'type'] = 'workshop'\n",
    "proceedings_data['edition'] = proceedings_data['edition'].astype(int)\n",
    "paper_in_proceedings = proceedings_data[['paperId', 'proceedingsID']].rename(columns={'paperId': 'paperID'}).reset_index(drop=True)\n",
    "\n",
    "paper_in_proceedings.info()\n",
    "paper_in_proceedings.to_csv('paper_presented_at_proceedings.csv', index=False)\n",
    "\n",
    "proceedings_data.drop_duplicates(subset=['proceedingsID'], inplace=True)\n",
    "proceedings_data['venue'] = np.random.choice(random_cities, size=len(proceedings_data))\n",
    "proceedings_data[['startDate', 'endDate']] = proceedings_data.apply(lambda x: generate_dates(x['year']), axis=1, result_type=\"expand\")\n",
    "proceedings_data = proceedings_data[['proceedingsID', 'edition', 'conferenceID', 'type', 'venue', 'startDate', 'endDate']]\n",
    "proceedings_data.info()\n",
    "proceedings_data.to_csv('proceedings.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writes 540478\n",
      "Paper in Proc 39026\n",
      "Paper in Vol 25952\n",
      "Proc 5007\n",
      "Authors 241051\n",
      "Papers 120789\n"
     ]
    }
   ],
   "source": [
    "writes = pd.read_csv('writes.csv')\n",
    "paper_presented_at_proceedings = pd.read_csv('paper_presented_at_proceedings.csv')\n",
    "proceedings = pd.read_csv('proceedings.csv')\n",
    "paper_published_in_volume = pd.read_csv('paper_published_in_volume.csv')\n",
    "authors = pd.read_csv('authors.csv')\n",
    "papers = pd.read_csv('papers.csv')\n",
    "\n",
    "filtered_paper_published_in_volume = paper_published_in_volume.sample(5000)\n",
    "\n",
    "print('Writes', writes.shape[0])\n",
    "print('Paper in Proc', paper_presented_at_proceedings.shape[0])\n",
    "print('Paper in Vol', paper_published_in_volume.shape[0])\n",
    "print('Proc', proceedings.shape[0])\n",
    "print('Authors', authors.shape[0])\n",
    "print('Papers', papers.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256566\n",
      "1137\n",
      "31233\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Count the number of editions each author has published in for each conference\n",
    "author_editions_count = (\n",
    "    writes.merge(paper_presented_at_proceedings, on=\"paperID\")\n",
    "    .merge(proceedings, on=\"proceedingsID\")\n",
    "    .groupby([\"authorID\", \"conferenceID\"])\n",
    "    .agg({\"edition\": \"nunique\"})\n",
    "    .reset_index()\n",
    ")\n",
    "# Step 2: Filter out authors who have published in less than 4 editions of any conference\n",
    "author_editions_count_filtered = author_editions_count.groupby('authorID').filter(lambda x: (x['edition'] >= 4).any())\n",
    "# Step 3: Get the list of authors to keep\n",
    "authors_to_keep = author_editions_count_filtered['authorID'].unique()\n",
    "# Step 4: Filter out data points associated with authors not in authors_to_keep\n",
    "papers_to_keep_from_editions = writes[writes['authorID'].isin(authors_to_keep)]['paperID']\n",
    "# New Step 5: Include papers that have been published in a volume\n",
    "papers_to_keep_from_volumes = paper_published_in_volume['paperID'].unique()\n",
    "# New Step 6: Combine papers to keep from both criteria\n",
    "papers_to_keep = pd.Series(np.union1d(papers_to_keep_from_editions, papers_to_keep_from_volumes))\n",
    "# Step 7: Filter out data points associated with papers to keep\n",
    "proceedings_to_keep = paper_presented_at_proceedings[paper_presented_at_proceedings['paperID'].isin(papers_to_keep)]['proceedingsID']\n",
    "# Now you can filter your original data frames using papers_to_keep and proceedings_to_keep\n",
    "filtered_writes = writes[writes['paperID'].isin(papers_to_keep)]\n",
    "filtered_proceedings = proceedings[proceedings['proceedingsID'].isin(proceedings_to_keep)]\n",
    "filtered_paper_presented_at_proceedings = paper_presented_at_proceedings[paper_presented_at_proceedings['proceedingsID'].isin(proceedings_to_keep)]\n",
    "\n",
    "print(filtered_writes.shape[0])\n",
    "print(filtered_proceedings.shape[0])\n",
    "print(filtered_paper_presented_at_proceedings.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_writes = filtered_writes.sample(10000)\n",
    "# filtered_paper_presented_at_proceedings = filtered_paper_presented_at_proceedings.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120789 8984\n",
      "241051 8344\n",
      "39026 2680\n"
     ]
    }
   ],
   "source": [
    "filtered_papers = filtered_writes['paperID'].unique()\n",
    "filtered_authors = filtered_writes['authorID'].unique()\n",
    "\n",
    "papers_new = papers[papers['paperID'].isin(filtered_papers)]\n",
    "authors_new = authors[authors['authorID'].isin(filtered_authors)]\n",
    "\n",
    "filtered_paper_presented_at_proceedings = filtered_paper_presented_at_proceedings[\n",
    "    filtered_paper_presented_at_proceedings[\"paperID\"].isin(filtered_papers)\n",
    "]\n",
    "\n",
    "print(papers.shape[0], papers_new.shape[0])\n",
    "print(authors.shape[0], authors_new.shape[0])\n",
    "print(paper_presented_at_proceedings.shape[0], filtered_paper_presented_at_proceedings.shape[0])\n",
    "\n",
    "papers_new.to_csv('papers_new.csv', index=False)\n",
    "authors_new.to_csv('authors_new.csv', index=False)\n",
    "filtered_writes.to_csv('writes_new.csv', index=False)\n",
    "filtered_proceedings.to_csv('proceedings_new.csv', index=False)\n",
    "filtered_paper_presented_at_proceedings.to_csv('paper_presented_at_proceedings_new.csv', index=False)\n",
    "filtered_paper_published_in_volume.to_csv('paper_published_in_volume_new.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605c2496f59b49f181b0c6195178a47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8986 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31428 entries, 0 to 31427\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   paperID   31428 non-null  object\n",
      " 1   authorID  31428 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 491.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperID</th>\n",
       "      <th>authorID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bc5921e71c5d3e1c7b39364244987c109a212</td>\n",
       "      <td>2237630226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000bc5921e71c5d3e1c7b39364244987c109a212</td>\n",
       "      <td>120625177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000bc5921e71c5d3e1c7b39364244987c109a212</td>\n",
       "      <td>2166262174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00102916233d568656659b8263efb6d8fb4a46d6</td>\n",
       "      <td>2237630226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00102916233d568656659b8263efb6d8fb4a46d6</td>\n",
       "      <td>120625177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    paperID    authorID\n",
       "0  000bc5921e71c5d3e1c7b39364244987c109a212  2237630226\n",
       "1  000bc5921e71c5d3e1c7b39364244987c109a212   120625177\n",
       "2  000bc5921e71c5d3e1c7b39364244987c109a212  2166262174\n",
       "3  00102916233d568656659b8263efb6d8fb4a46d6  2237630226\n",
       "4  00102916233d568656659b8263efb6d8fb4a46d6   120625177"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a map of paperID to its authors to speed up lookup\n",
    "paper_authors_map = filtered_writes.groupby('paperID')['authorID'].apply(set).to_dict()\n",
    "\n",
    "# Prepare a list to collect review assignments\n",
    "review_assignments = []\n",
    "\n",
    "# Iterate over paperIDs to assign reviewers\n",
    "for paper_id, authors in tqdm(paper_authors_map.items()):\n",
    "    # Find eligible reviewers: all authors excluding those who wrote the paper\n",
    "    eligible_reviewers = authors_new[~authors_new['authorID'].isin(authors_new)]\n",
    "    num_reviewers = np.random.choice([2, 3, 4, 5])\n",
    "    # Ensure there are enough eligible reviewers\n",
    "    if len(eligible_reviewers) >= num_reviewers:\n",
    "        # Randomly select 3 reviewers without replacement\n",
    "        selected_reviewers = eligible_reviewers.sample(n=num_reviewers, random_state=1)['authorID'].tolist()\n",
    "        for author_id in selected_reviewers:\n",
    "            review_assignments.append({'paperID': paper_id, 'authorID': author_id})\n",
    "\n",
    "# Convert the list of review assignments into a DataFrame\n",
    "reviews = pd.DataFrame(review_assignments)\n",
    "reviews.to_csv('reviews_new.csv', index=False)\n",
    "reviews.info()\n",
    "\n",
    "# Display the head of the optimized reviews DataFrame\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = papers_new['year'].unique()\n",
    "with open(\"years.csv\", \"w\") as f:\n",
    "    f.write(\"year\\n\")\n",
    "    for year in years:\n",
    "        f.write(str(year) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404217, 2) (3288, 2)\n"
     ]
    }
   ],
   "source": [
    "# filter citations to only keep those for which paperId is in filtered_papers\n",
    "citations = pd.read_csv(\"citations.csv\")\n",
    "filtered_citations = citations[\n",
    "    citations[\"paperID\"].isin(papers_new[\"paperID\"])\n",
    "    & citations[\"referenceID\"].isin(papers_new[\"paperID\"])\n",
    "]\n",
    "print(citations.shape, filtered_citations.shape)\n",
    "filtered_citations.to_csv(\"citations_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(362301, 2) (26950, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h9/fg7r3_v97d5916_gtz3qfnl40000gn/T/ipykernel_41229/1795590332.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_keyword_mapping['keywords'] = filtered_keyword_mapping['keywords'].str.lower()\n"
     ]
    }
   ],
   "source": [
    "keyword_mapping = pd.read_csv('keyword_mapping.csv')\n",
    "filtered_keyword_mapping = keyword_mapping[keyword_mapping['paperId'].isin(papers_new['paperID'])]\n",
    "filtered_keyword_mapping['keywords'] = filtered_keyword_mapping['keywords'].str.lower()\n",
    "print(keyword_mapping.shape, filtered_keyword_mapping.shape)\n",
    "filtered_keyword_mapping.to_csv('keyword_mapping_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample review contents to choose from\n",
    "review_contents = [\n",
    "    \"This paper presents a novel approach that shows promising results.\",\n",
    "    \"The methodology is sound, but the paper lacks sufficient experimental validation.\",\n",
    "    \"Excellent work! The results are well presented and clearly support the conclusions.\",\n",
    "    \"The paper is well-written, but the relevance to the field is not clearly established.\",\n",
    "    \"Significant contribution to the field, but the study lacks originality.\",\n",
    "    \"The experimental section is thorough, but the paper is overly verbose.\",\n",
    "    \"Interesting approach, but the paper does not provide enough context for the results.\",\n",
    "    \"The paper could be improved by adding more details about the data collection process.\",\n",
    "    \"Solid work, but the analysis lacks depth in some areas.\",\n",
    "    \"The paper addresses an important problem, but the solution is not convincingly better than existing methods.\"\n",
    "]\n",
    "\n",
    "reviews['content'] = np.random.choice(review_contents, size=len(reviews))\n",
    "reviews['decision'] = np.random.choice([True, False], size=len(reviews))\n",
    "\n",
    "reviews.head()\n",
    "reviews.to_csv('reviews_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "conferences = pd.read_csv('conferences.csv')\n",
    "workshops = pd.read_csv('workshops.csv')\n",
    "journals = pd.read_csv('journals.csv')\n",
    "\n",
    "reviewer_policy_choices = [2, 3, 4, 5]\n",
    "\n",
    "conferences['reviewerPolicy'] = np.random.choice(reviewer_policy_choices, size=len(conferences))\n",
    "workshops['reviewerPolicy'] = np.random.choice(reviewer_policy_choices, size=len(workshops))\n",
    "journals['reviewerPolicy'] = np.random.choice(reviewer_policy_choices, size=len(journals))\n",
    "\n",
    "conferences.to_csv('conferences.csv', index=False)\n",
    "workshops.to_csv('workshops.csv', index=False)\n",
    "journals.to_csv('journals.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orgID</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O1</td>\n",
       "      <td>Tech Innovations Inc.</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O2</td>\n",
       "      <td>Global Research Ltd.</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O3</td>\n",
       "      <td>Future Technologies LLC</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O4</td>\n",
       "      <td>Quantum Computing Corp.</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O5</td>\n",
       "      <td>AI Solutions Inc.</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  orgID                     name     type\n",
       "0    O1    Tech Innovations Inc.  company\n",
       "1    O2     Global Research Ltd.  company\n",
       "2    O3  Future Technologies LLC  company\n",
       "3    O4  Quantum Computing Corp.  company\n",
       "4    O5        AI Solutions Inc.  company"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a synthetic dataset for organizations\n",
    "organization_names = [\n",
    "    \"Tech Innovations Inc.\", \"Global Research Ltd.\", \"Future Technologies LLC\",\n",
    "    \"Quantum Computing Corp.\", \"AI Solutions Inc.\", \"Renewable Energy Systems Ltd.\",\n",
    "    \"Deep Learning Technologies LLC\", \"Space Exploration Corp.\", \"Biotech Innovations Inc.\",\n",
    "    \"Cyber Security Solutions Ltd.\", \"University of Science and Technology\", \"Global University of Engineering\",\n",
    "    \"Institute of Advanced Studies\", \"National University of Arts and Sciences\", \n",
    "    \"International College of Information Technology\", \"University of Renewable Energies\",\n",
    "    \"Institute for Space Research\", \"University of Biotech Innovations\",\n",
    "    \"College of Quantum Computing\", \"Academy of Cyber Security\"\n",
    "]\n",
    "organization_types = [\"company\"] * 10 + [\"university\"] * 10\n",
    "organizations_data = {\n",
    "    'orgID': [f\"O{i+1}\" for i in range(len(organization_names))],\n",
    "    'name': organization_names,\n",
    "    'type': organization_types\n",
    "}\n",
    "organizations_df = pd.DataFrame(organizations_data)\n",
    "\n",
    "authors = pd.read_csv('authors_new.csv')\n",
    "# Assign a random organization to each author\n",
    "authors['affiliation'] = np.random.choice(organizations_df['orgID'], size=len(authors))\n",
    "\n",
    "authors.to_csv('authors_new.csv', index=False)\n",
    "organizations_df.to_csv('organizations.csv', index=False)\n",
    "\n",
    "organizations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph                     107\n",
      "graph neural               52\n",
      "knowledge graph            20\n",
      "knowledge graphs           18\n",
      "graph convolutional        11\n",
      "                         ... \n",
      "subgraph pattern            1\n",
      "graph structured            1\n",
      "subgraph queries            1\n",
      "geographic information      1\n",
      "graph theories              1\n",
      "Name: count, Length: 214, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"keyword_mapping_new.csv\")\n",
    "\n",
    "# Assuming the column containing keywords is named 'keywords'\n",
    "# Split the keywords column into a list of keywords\n",
    "keywords_list = df['keywords'].str.split(',')\n",
    "\n",
    "# Flatten the list of lists into a single list of keywords\n",
    "all_keywords = [keyword.strip() for sublist in keywords_list for keyword in sublist]\n",
    "\n",
    "# Create a frequency table of keywords containing the word \"graph\"\n",
    "graph_keywords = [keyword for keyword in all_keywords if 'graph' in keyword.lower()]\n",
    "graph_freq_table = pd.Series(graph_keywords).value_counts()\n",
    "\n",
    "# Display the frequency table\n",
    "print(graph_freq_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['graph', 'graph neural', 'knowledge graph', 'knowledge graphs', 'graph convolutional', 'graph embedding']\n"
     ]
    }
   ],
   "source": [
    "print(graph_freq_table.head(6).index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdm1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
